                                        #An Empirical Comparison of Two Load Balancing Algorithms

##Project Description
	Web-based systems often use multiple servers to manage client requests. As a result, load balancing, or the process of allocating requests to these servers, is an important part of system design. Load balancing algorithms include the “round robin” algorithm, which assigns requests to servers sequentially, as well as more complex algorithms that take into account differing server capabilities [1]. This project builds a model of a distributed server system and implements a load balancer: a centralized server that receives all client requests and distributes these requests to the system’s servers according to a load balancing algorithm. We compare two algorithms: (1) round robin and (2) a weighted state-based algorithm that takes into account the load that has already been assigned to a given server when assigning new requests to that server. 
Our project models a specific scenario: a website with 5 servers receives many requests from its users simultaneously. This scenario appears in situations such as ticket selling, where users compete to acquire tickets for a popular concert and all try to order tickets simultaneously. There are three goals for the website. First, it hopes to serve all clients in a timely manner, so that all clients receive a response from the server quickly. Next, it hopes to ensure fairness: users should all be waiting for a similar amount of time for their request to be served, rather than some users being served much more quickly than others. Finally, the website wants to use its server resources effectively and ensure that the resource allocation is balanced across all servers, taking into account the varying server compute power.  In line with similar work, we evaluate our system in terms of the response time and the load assigned to each server, in order to determine if there is an advantage in using one algorithm over another in this scenario [2].

##Design Overview
The load balancing algorithm we designed aims to achieve strategic resource allocation. If one server is faster than all others, but is assigned an equal number of tasks as all other servers, then there is a missed opportunity to take advantage of that server’s efficiency. In the context of HTTP requests sent over the internet, there is a high motivation to ensure efficient responses in order to avoid impatient clients resending requests due to an overly long server response time.
To simplify the implementation, the servers and the requests are modeled as types. This allows us to simulate the server properties (number of cores, server request queue) and the request properties (request id, load). Servers complete requests by waiting for a set amount of time determined by the request’s load and the number of cores available to the server. The requests are sent to the load balancer, which assigns requests to servers. Concurrently, the servers execute requests already present in the server’s queue. 
The load balancing system addresses the question of resource allocation. Server pools usually contain different models with different numbers of cores. At the same time, incoming requests may have different loads (meaning they require different quantities of resources). Our system implements two paradigms for matching requests to servers. The first, “round robin,” distributes requests equally across all servers, regardless of the request or server in question. This is done by simply keeping track of the last server that was assigned a request– it has very little overhead, but doesn’t optimize at all. The other algorithm, “state-based,” distributes requests based on the current state of all servers. For each server, it checks the total load size of the request queue weighted by the number of cores (load divided by cores) and assigns the request to the server with the smallest relative load. This ensures that servers with more cores or lighter requests get more requests, while servers with fewer cores or heavier requests get fewer requests.
This system also must be designed using concurrency. Different servers have to run at the same time, otherwise the advantage of having multiple servers would be lost. But servers also access the same data– anytime a server finishes handling a request, it deletes that request from the balancers ‘waiting for acknowledgement’ array, indicating that the request was successfully handled and the balancer doesn’t need to worry about it anymore. To prevent two servers from accessing the balancer at the same time, the balancer had to be designed with a Mutex object, which was locked every time a server began accessing the balancer’s metadata and unlocked once it finished.

##Testing Methodology
We tested two scenarios to evaluate how the system performs as the number of requests increases  from 100 to 1000 requests. In both scenarios, the system uses 5 servers, with varying compute power. In the first scenario, the load for each request is held constant. In the second scenario, the load for each request is randomly assigned. To produce the plots, test results were formatted in CSV files and imported into Jupyter Notebook. The pandas library in Python was used to create all plots.
We evaluate the load balancing algorithms according to two metrics. First, we consider the response time, which represents the amount of time a user would have to wait to receive a response to their request. For each scenario, we plot the average response time (Fig. 1, Fig. 2). We also plot the coefficient of variation, which is the ratio of the standard deviation of the response time to the average response time, in order to compare the variability in response time between the two algorithms as the number of requests increases (Fig. 3, Fig. 4). 
 	The second metric we consider is the weighted load per server. Since each server has a different number of cores, this metric determines the average load handled by each server’s core, allowing us to compare the resource allocation across servers. We plot the average of the weighted load per server using the two scenarios described above (a fixed request load and a randomized request load), while varying the number of requests (Fig. 5, Fig. 6). We also plot the coefficient of variation for the weighted load per server in order to compare the variability in this metric between the two algorithms (Fig. 7, Fig. 8).
		
##Outcome
With a fixed task load, the state-based algorithm outperforms the round robin algorithm (Fig. 1). With a state-based algorithm, the mean response time increases slowly as the number of requests increases, while round robin gets continually slower at a faster rate. However, when considering the coefficient of variation for this metric, we see that round robin has a smaller coefficient that remains fairly constant, compared to the state-based algorithm which overall has a higher coefficient of variation compared to round robin (Fig. 3). This means that if the state-based algorithm is used for a fixed task size, users will have faster response times, but users cannot expect to wait an equal amount of time compared to their peers, which prevents fairness in the user experience. In contrast, when the task loads are randomized, the algorithms perform similarly, with the overhead of the state-based algorithm leading to slightly higher average response times (Fig. 2). When considering the weighted server load, only the state-based algorithm successfully distributed tasks evenly across servers in both cases, as seen by the near-zero coefficient of variance (Fig. 7, Fig. 8). Therefore, while in the randomized setting either algorithm would produce the same experience from the client’s perspective, using the state-based algorithm would aid a website in ensuring optimal resource utilization by taking into account the different server capabilities. Therefore, if a website has different types of servers, it would benefit from using the state-based algorithm. However, if the website’s servers are all similar, the simplicity of round robin will achieve similar results to the more involved state-based algorithm. 
Figures



Figure 1: Mean Response Time vs. Number of Tasks, Fixed Task Size
Figure 2: Mean Response Time vs. Number of Tasks, Randomized Task Size


Figure 3: Response Time Coefficient of Variation vs. Number of Tasks, Fixed Task Size
Figure 4: Response Time Coefficient of Variation vs. Number of Tasks, Randomized Task Size


Figure 5: Mean Weighted Load per Server vs. Number of Tasks, Fixed Task Size
Figure 6: Mean Weighted Load per Server vs. Number of Tasks, Randomized Task Size


Figure 7: Weighted Load per Server Coefficient of Variation vs. Number of Tasks, Fixed Task Size
Figure 8: Weighted Load per Server Coefficient of Variation vs. Number of Tasks, Randomized Task Size


Honor Code
This project represents our own work in accordance with University regulations.
/s/ Sophie Goldman
/s/ Ari Brown

Acknowledgements
Thank you to Neil Agarwal for introducing us to load balancing algorithms and for his support during the project.

Bibliography

[1] “Algorithms for making load-balancing decisions,” IBM, Dec. 14, 2021. https://prod.ibmdocs-production-dal-6099123ce774e592a519d7c33db8265e-0000.us-south.containers.appdomain.cloud/docs/en/datapower-gateway/10.0.x?topic=groups-algorithms-making-load-balancing-decisions (accessed Dec. 16, 2022).

[2] R. D. Tedja, I. M. Murwantara, and P. Yugopuspito, “Comparing Load-Balancer Algorithms Performance on Virtualized Environment,” in 2021 8th International Conference on Information Technology, Computer and Electrical Engineering (ICITACEE), 2021, pp. 155–160. doi: 10.1109/ICITACEE53184.2021.9617528.








